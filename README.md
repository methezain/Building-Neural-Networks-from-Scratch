# ðŸ§  Building Neural Networks from Scratch

This project is the result of my journey through the foundational concepts of Artificial Neural Networks (ANNs), built entirely from scratch with Python.

---

## ðŸ“š What I Learned

- **Python Libraries**  
  - Explored and used core libraries like `NumPy` and `Pandas`.

- **Neural Network Fundamentals**  
  - Understood the concept of a **neuron**, how it processes input, and passes output.
  - Built an understanding of **layers of neurons** â€” input, hidden, and output layers.

- **Data Flow in ANNs**  
  - Learned how **input data** flows through a network.
  - Comprehended how **outputs** are generated from neurons.

- **Forward Propagation**  
  - Understood the step-by-step computation of predictions via the **forward pass**.

- **Activation Functions**
  - One of the most interesting things in ANNs.
  - Understood the concepts of ADAGRAD, RMPProp, and ADAM

- **Backward Propagation**  
  - Learned how the network adjusts its weights through **backpropagation**.

- **Mathematical Building Blocks**  
  - Mastered the use of:
    - **Dot products** for weighted sums
    - **Activation functions** (like ReLU, Sigmoid)
    - **Partial derivatives** for gradient calculations

- **Loss Function**  
  - Understood how the **loss function** measures prediction error.

---

## âœ… What I Achieved

- Successfully ran a simple neural network implementation from scratch.
- Built a strong foundation in both **theory and code** for Artificial Neural Networks.
- Developed a deeper understanding of the **math behind machine learning**.

---

Thanks for reading! Feel free to explore the code and share feedback.
